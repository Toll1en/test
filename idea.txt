from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

import mlflow
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize

try:
    import hdbscan  # type: ignore
except Exception:
    hdbscan = None


@dataclass
class ClusterResult:
    assignment: pd.DataFrame  # multi-assignments (one row per (text_key, cluster_rank))
    labels: pd.Series         # hard label per unique row (fallback)
    n_groups: int


def _cosine_similarity_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    A_n = normalize(A, axis=1)
    B_n = normalize(B, axis=1)
    return A_n @ B_n.T


def _log_cluster_config(
    algorithm: str,
    group_cols: list[str],
    allow_multicluster: bool,
    top_k: int,
    score_threshold: float | None,
    min_group_size: int,
    compute_silhouette: bool,
    silhouette_sample: int,
    kmeans_k: int,
    random_state: int,
    hdbscan_cfg: dict[str, Any],
) -> None:
    # metrics = numbers, params = strings/bools
    mlflow.log_param("clust_algorithm", algorithm)
    mlflow.log_param("clust_group_cols", ",".join(group_cols) if group_cols else "ALL")
    mlflow.log_param("clust_allow_multicluster", bool(allow_multicluster))
    mlflow.log_param("clust_top_k", int(top_k))
    mlflow.log_param("clust_score_threshold", score_threshold)
    mlflow.log_param("clust_min_group_size", int(min_group_size))
    mlflow.log_param("clust_compute_silhouette", bool(compute_silhouette))
    mlflow.log_param("clust_silhouette_sample", int(silhouette_sample))

    if algorithm == "kmeans":
        mlflow.log_param("kmeans_k", int(kmeans_k))
        mlflow.log_param("kmeans_random_state", int(random_state))

    if algorithm == "hdbscan":
        for k, v in hdbscan_cfg.items():
            mlflow.log_param(f"hdbscan_{k}", v)


def cluster_texts(
    df_unique: pd.DataFrame,
    embeddings: np.ndarray,
    key_col: str = "TEXT_KEY_CLEAN",
    group_cols: Optional[list[str]] = None,
    algorithm: str = "kmeans",
    kmeans_k: int = 10,
    random_state: int = 42,
    min_group_size: int = 15,
    allow_multicluster: bool = True,
    top_k: int = 3,
    score_threshold: float | None = 0.25,
    similarity: str = "cosine",
    compute_silhouette: bool = True,
    silhouette_sample: int = 2000,
    hdbscan_cfg: Optional[dict[str, Any]] = None,
) -> ClusterResult:
    """
    Clustering für df_unique + embeddings.
    - Unterstützt KMeans und HDBSCAN.
    - Gibt Multi-Assignments zurück (cluster_rank/cluster_score).
    - labels = hard label pro unique row (für schnellen Merge/Fallback).
    """

    if embeddings.ndim != 2:
        raise ValueError("embeddings muss 2D sein.")
    if len(df_unique) != embeddings.shape[0]:
        raise ValueError("df_unique und embeddings müssen gleich lang sein.")
    if key_col not in df_unique.columns:
        raise ValueError(f"{key_col} fehlt in df_unique.")

    group_cols = group_cols or []
    hdbscan_cfg = hdbscan_cfg or {}

    algorithm = algorithm.lower().strip()
    if algorithm not in {"kmeans", "hdbscan"}:
        raise ValueError(f"Unbekannter algorithm: {algorithm}")

    if algorithm == "hdbscan" and hdbscan is None:
        raise ImportError("hdbscan ist nicht installiert. Bitte: pip install hdbscan")

    # hard labels pro unique row
    hard_labels = pd.Series([-1] * df_unique.shape[0], index=df_unique.index, name="cluster_id")

    rows: list[dict[str, Any]] = []
    silhouette_values: list[float] = []

    # Gruppen definieren
    if group_cols:
        for c in group_cols:
            if c not in df_unique.columns:
                raise ValueError(f"group col fehlt: {c}")
        groups = df_unique.groupby(group_cols, dropna=False, sort=False)
        n_groups = int(groups.ngroups)
    else:
        groups = [(("ALL",), df_unique)]
        n_groups = 1

    mlflow.log_metric("clust_groups_total", float(n_groups))
    _log_cluster_config(
        algorithm=algorithm,
        group_cols=group_cols,
        allow_multicluster=allow_multicluster,
        top_k=top_k,
        score_threshold=score_threshold,
        min_group_size=min_group_size,
        compute_silhouette=compute_silhouette,
        silhouette_sample=silhouette_sample,
        kmeans_k=kmeans_k,
        random_state=random_state,
        hdbscan_cfg=hdbscan_cfg,
    )

    clustered_groups = 0
    skipped_groups = 0

    for group_key, g in groups:
        idx = g.index.to_numpy()
        n = len(idx)

        if n < min_group_size:
            skipped_groups += 1
            for i in idx:
                rows.append({
                    key_col: df_unique.loc[i, key_col],
                    "cluster_id": None,
                    "cluster_score": None,
                    "cluster_rank": None,
                })
            continue

        X = embeddings[df_unique.index.get_indexer(idx)]

        # ---- Fit algorithm ----
        labels: np.ndarray
        centers: Optional[np.ndarray] = None  # for similarity-based multi assignment
        if algorithm == "kmeans":
            k = min(kmeans_k, n)
            if k < 2:
                skipped_groups += 1
                for i in idx:
                    rows.append({
                        key_col: df_unique.loc[i, key_col],
                        "cluster_id": None,
                        "cluster_score": None,
                        "cluster_rank": None,
                    })
                continue

            km = KMeans(n_clusters=k, random_state=random_state, n_init="auto")
            labels = km.fit_predict(X)
            centers = km.cluster_centers_
            clustered_groups += 1

        else:  # hdbscan
            # defaults
            min_cluster_size = int(hdbscan_cfg.get("min_cluster_size", 10))
            min_samples = hdbscan_cfg.get("min_samples", None)
            metric = str(hdbscan_cfg.get("metric", "euclidean"))
            cluster_selection_method = str(hdbscan_cfg.get("cluster_selection_method", "eom"))

            # Fit HDBSCAN
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric=metric,
                cluster_selection_method=cluster_selection_method,
            )
            labels = clusterer.fit_predict(X)  # noise = -1
            clustered_groups += 1

            # pseudo-centers: mean vector per cluster (für Multi-Assignment / Scores)
            unique_clusters = [c for c in np.unique(labels) if c != -1]
            if len(unique_clusters) > 0:
                centers = np.vstack([X[labels == c].mean(axis=0) for c in unique_clusters])
            else:
                centers = None

        # hard labels setzen (auch -1 ist ok)
        for pos, i in enumerate(idx):
            hard_labels.loc[i] = int(labels[pos])

        # ---- Multi assignment based on similarity to centers ----
        if similarity != "cosine":
            raise ValueError("nur cosine wird unterstützt (für multi assignment)")

        if centers is None or centers.shape[0] == 0:
            # keine Cluster gefunden (z.B. alles noise bei hdbscan)
            for i in idx:
                rows.append({
                    key_col: df_unique.loc[i, key_col],
                    "cluster_id": None,
                    "cluster_score": None,
                    "cluster_rank": None,
                })
        else:
            sims = _cosine_similarity_matrix(X, centers)

            for pos, i in enumerate(idx):
                sim_row = sims[pos]
                order = np.argsort(-sim_row)

                chosen = order[:max(1, top_k)] if allow_multicluster else order[:1]

                rank = 1
                wrote_any = False
                for cpos in chosen:
                    score = float(sim_row[cpos])
                    if score_threshold is not None and score < score_threshold:
                        continue

                    # cluster_id mapping:
                    # - for kmeans: center index == cluster id
                    # - for hdbscan: center index maps to unique_clusters[cpos]
                    if algorithm == "kmeans":
                        cluster_id = int(cpos)
                    else:
                        # rebuild same ordering as centers creation
                        unique_clusters = [c for c in np.unique(labels) if c != -1]
                        cluster_id = int(unique_clusters[cpos])

                    rows.append({
                        key_col: df_unique.loc[i, key_col],
                        "cluster_id": cluster_id,
                        "cluster_score": score,
                        "cluster_rank": int(rank),
                    })
                    wrote_any = True
                    rank += 1

                if not wrote_any:
                    # fallback: best center
                    best = int(order[0])
                    if algorithm == "kmeans":
                        cluster_id = best
                    else:
                        unique_clusters = [c for c in np.unique(labels) if c != -1]
                        cluster_id = int(unique_clusters[best])

                    rows.append({
                        key_col: df_unique.loc[i, key_col],
                        "cluster_id": cluster_id,
                        "cluster_score": float(sim_row[best]),
                        "cluster_rank": 1,
                    })

        # ---- Evaluation (Silhouette) ----
        if compute_silhouette:
            # For HDBSCAN: ignore noise label -1
            if algorithm == "hdbscan":
                mask = labels != -1
                X_eval = X[mask]
                y_eval = labels[mask]
            else:
                X_eval = X
                y_eval = labels

            unique_labels = np.unique(y_eval)
            if len(unique_labels) >= 2:
                n_eval = X_eval.shape[0]
                if n_eval > silhouette_sample:
                    rng = np.random.RandomState(random_state)
                    sample_idx = rng.choice(n_eval, size=silhouette_sample, replace=False)
                    sil = silhouette_score(X_eval[sample_idx], y_eval[sample_idx], metric="cosine")
                else:
                    sil = silhouette_score(X_eval, y_eval, metric="cosine")
                silhouette_values.append(float(sil))

    assignment = pd.DataFrame(rows)

    mlflow.log_metric("clust_groups_clustered", float(clustered_groups))
    mlflow.log_metric("clust_groups_skipped", float(skipped_groups))

    share_none = float(assignment["cluster_id"].isna().mean()) if len(assignment) else 1.0
    mlflow.log_metric("clust_share_no_cluster_rows", float(share_none))

    if silhouette_values:
        mlflow.log_metric("silhouette_mean", float(np.mean(silhouette_values)))
        mlflow.log_metric("silhouette_min", float(np.min(silhouette_values)))
        mlflow.log_metric("silhouette_max", float(np.max(silhouette_values)))
    else:
        mlflow.log_metric("silhouette_mean", 0.0)

    return ClusterResult(assignment=assignment, labels=hard_labels, n_groups=n_groups)

3) Wie du es in der Pipeline aufrufst

In pipeline.py ungefähr so:

from components.clustering import cluster_texts

cl_cfg = config["clustering"]

res = cluster_texts(
    df_unique=df_unique,
    embeddings=embeddings,
    key_col=config["preprocessing"]["dedup_key_col"],  # oder "TEXT_KEY_CLEAN"
    group_cols=cl_cfg.get("group_cols"),
    algorithm=cl_cfg["algorithm"],
    kmeans_k=cl_cfg.get("kmeans_k", 10),
    random_state=cl_cfg.get("random_state", 42),
    min_group_size=cl_cfg.get("min_group_size", 15),
    allow_multicluster=cl_cfg.get("allow_multicluster", True),
    top_k=cl_cfg.get("top_k", 3),
    score_threshold=cl_cfg.get("score_threshold", 0.25),
    compute_silhouette=cl_cfg.get("compute_silhouette", True),
    silhouette_sample=cl_cfg.get("silhouette_sample", 2000),
    hdbscan_cfg=cl_cfg.get("hdbscan", {}),
)